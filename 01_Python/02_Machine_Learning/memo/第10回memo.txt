ニューラルネットワーク

サポートベクタマシンという手法

特徴
マージン最大化を行う
ソフトマージンに対応できる
カーネル法を使える


データセットを作成する。データを作ることも今後必要。

make_blobs：
sklearnの中にある、データを生成するメゾッド
引数一覧：
n_samples：作るデータの個数
n_features：何次元のデータを作るか（特徴量の数）
centers：中心点の配列
cluster_std：標準偏差（散らばり具合）ここを小さくするときゅっとまとまったデータができる。
ramdom_state：t乱数のたね（0だと毎回同じデータができる）


train_test_split
データセットを教師データ（学習データ）と未知データ（テストデータ）に分ける。
test_size：未知データの割合。


より良い決定境界を引くには？
サポートベクタマシンの方法が役にたつ！

交差検証：
データの分割と評価を何度も繰り返す


決定境界に最も近い点を通り、決定境界に並行な線を引くとする。
この並行な線と決定境界の幅がマージン。
マージンが最大になるように、最適化された位置に決定境界をひく。
（並行な線に乗っている点が、サポートベクタ（サポートベクトル）と言う。


線形SVM=SVC(kernel='linear',C=sys.maxsize) # 誤分類には厳しく
線形SVM.fit(学習データのX,学習データのラベルy) # 学習完了
SVC：sklearnのメゾッド
linear：まっすぐな線で分類する、ということ。
Fit：学習完了


線形SVM.conf_
coeffisient：係数
1次関数で表せるため、ax + by のa,bがこれに相当。決定境界の式の係数。


=================================================================



線形SVM.support_vectors_
サポートベクタが二次元配列で格納されている。（学習済であること）


線形SVM.predict(テストデータのX)
テストデータへの分類結果（テストデータのy）を予測する



accuracy_score(テストデータのラベルy,テストデータの分類結果) # 正解率




【ソフトマージン】

線形SVM=SVC(kernel='linear',C=sys.maxsize)

線形SVM=SVC(kernel='linear',C=5) # ペナルティCを小さくする

C=sys.maxsize（無限大）だと、誤分類に厳しい。
C=5のように小さくすると、誤分類を許す。（マージンの内側にデータが入り込むことを許す）
← →ハードマージン
ノイズに強い性質「ノイズに対するロバスト性」

音声認識（ノイズが入りやすい）や、道路標識の識別（シールや葉っぱなどノイズが入る）などに応用。

※Cの値（ペナルティのパラメタ）は0以下は許されない。


【カーネル法】
線形分離できない問題に強い。

from sklearn.datasets import make_gaussian_quantiles
同心円上に分けられたデータを作る　　　　　　　　　　　　　　

カーネル法のイメージは、
2次元のデータを3次元にして学習し、z軸で線形分離する

非線形SVM=SVC(kernel='rbf') # ガウシアンカーネル
→
ガウシアンカーネルの設定

kernel='rbf'		ガウシアンカーネル
kernel='linear'		線形カーネル
kernel='poly'		多項式カーネル
kernel='sigmoid'	シグモイドカーネル
扱うデータによって選べるようになっている。


